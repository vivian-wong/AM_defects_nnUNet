{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this file is used to put NIST's AM data into nnUNet's format. Please put this in the same folder as raw NIST data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nibabel.testing import data_path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from IPython.display import clear_output\n",
    "from skimage.transform import resize\n",
    "import sys\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_specimen_dict(specimen='S1S2'):\n",
    "    def initialize_rand(specimen): \n",
    "        # helper function to return a randomly initialized 3D matrix. \n",
    "        # get size \n",
    "        for root, dirs, files in os.walk(\"./data\"):\n",
    "            if specimen in root and \"h5_files\" not in root:\n",
    "                # read one file to get image size\n",
    "                files = sorted(files)\n",
    "                matrix = cv2.imread(os.path.join(os.path.abspath(root),files[1]),cv2.IMREAD_UNCHANGED)\n",
    "                if \"Bernsen\" in root:\n",
    "                    matrix[matrix==255]=1\n",
    "                    if matrix.max()>1:\n",
    "                        print(name)\n",
    "                tifCounter = len(glob.glob1(root,\"*.tif\"))\n",
    "                if (tifCounter != 900) and (tifCounter != 749):\n",
    "                    print(tifCounter)\n",
    "                assert (tifCounter == 900) or (tifCounter == 749)\n",
    "                break\n",
    "        return np.zeros((tifCounter, matrix.shape[0],matrix.shape[1]),dtype=np.uint8) # e.g. (900, 1010, 980)\n",
    "                        \n",
    "    d = {}\n",
    "    # key = 'raw' or 'label'\n",
    "    # value = np 3d array (0-255 or binary)\n",
    "    for root, dirs, files in os.walk(\"./data\"):\n",
    "        if specimen in root and \"h5_files\" not in root:\n",
    "            imgs = initialize_rand(specimen) # must re-initialize or else it'll change what has been stored.\n",
    "            files = sorted(files)\n",
    "            tifCounter = 0\n",
    "            for name in files: \n",
    "                if \".tif\" in name: \n",
    "                    path = os.path.join(os.path.abspath(root), name)\n",
    "                    matrix = cv2.imread(path,cv2.IMREAD_UNCHANGED) # all uint8 when read in\n",
    "                    \n",
    "                    if \"Bernsen\" in root:\n",
    "                        # convert to binary\n",
    "                        assert (imgs.max() <= 1)\n",
    "                        matrix[matrix==255]=1\n",
    "                        assert(matrix.max()==1)\n",
    "                    imgs[tifCounter,:,:] = matrix\n",
    "                    tifCounter += 1\n",
    "                    \n",
    "            assert(tifCounter == imgs.shape[0])\n",
    "            \n",
    "            if \"Med3D\" in root: # raw\n",
    "                d['raw'] = imgs\n",
    "            elif \"Bernsen\" in root: # label\n",
    "                d['label'] = imgs\n",
    "                d['label'] = 1-d['label'] # 0=>1 = black; 0 = white\n",
    "                \n",
    "    assert(d['label'].max()==1)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save NIFTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(A):\n",
    "    # normalize across entire 3d matrix\n",
    "    # convert to float32 to save memory\n",
    "    return ((A-A.min())/(A.max()-A.min())).astype(np.float32)\n",
    "\n",
    "# loop over all specimens and get 3d np array of each (label + raw)\n",
    "training_specs = ['S1S3','S1S4','S1S5'] #\n",
    "testing_specs = ['S1S2']\n",
    "output_path = '.' # change this according to need\n",
    "for spec in training_specs:\n",
    "    d = create_specimen_dict(spec)\n",
    "    raw_nii = nib.Nifti1Image(d['raw'], affine=np.eye(4))\n",
    "    label_nii = nib.Nifti1Image(d['label'], affine=np.eye(4))\n",
    "    nib.save(raw_nii, os.path.join(output_path, 'imagesTr/'+spec+'_0000.nii.gz'))\n",
    "    nib.save(label_nii, os.path.join(output_path, 'labelsTr/'+spec+'.nii.gz'))\n",
    "for spec in testing_specs:\n",
    "    d = create_specimen_dict(spec)\n",
    "    raw_nii = nib.Nifti1Image(d['raw'], affine=np.eye(4))\n",
    "    label_nii = nib.Nifti1Image(d['label'], affine=np.eye(4))\n",
    "    nib.save(raw_nii, os.path.join(output_path, 'imagesTs/'+spec+'_0000.nii.gz'))\n",
    "    nib.save(label_nii, os.path.join(output_path, 'labelsTs/'+spec+'.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'AM', 'description': 'NIST_AM for nnUNet', 'tensorImageSize': '3D', 'reference': '', 'licence': '', 'release': '0.0', 'modality': {'0': 'XCT'}, 'labels': ({'0': 'background', '1': 'defect'},), 'numTraining': 3, 'numTest': 1, 'training': [{'image': './images/S1S3_0000.nii.gz', 'label': './labels/S1S3.nii.gz'}, {'image': './images/S1S4_0000.nii.gz', 'label': './labels/S1S4.nii.gz'}, {'image': './images/S1S5_0000.nii.gz', 'label': './labels/S1S5.nii.gz'}], 'test': ['./images/S1S2.nii.gz']}\n"
     ]
    }
   ],
   "source": [
    "task_name = 'AM'\n",
    "training_specimen_names = ['S1S3','S1S4','S1S5']\n",
    "test_specimen_names = ['S1S2']\n",
    "\n",
    "json_dict = {}\n",
    "json_dict['name'] = task_name\n",
    "json_dict['description'] = \"NIST_AM for nnUNet\"\n",
    "json_dict['tensorImageSize'] = \"3D\"\n",
    "json_dict['reference'] = \"\"\n",
    "json_dict['licence'] = \"\"\n",
    "json_dict['release'] = \"0.0\"\n",
    "json_dict['modality'] = {\n",
    "    \"0\": \"XCT\",\n",
    "}\n",
    "json_dict['labels'] = {\n",
    "    \"0\": \"background\",\n",
    "    \"1\": \"defect\",\n",
    "}\n",
    "\n",
    "json_dict['numTraining'] = len(training_specimen_names)\n",
    "json_dict['numTest'] = len(test_specimen_names)\n",
    "json_dict['training'] = [{'image': \"./imagesTr/%s.nii.gz\" % i.split(\"/\")[-1], \"label\": \"./labelsTr/%s.nii.gz\" % i.split(\"/\")[-1]} for i in\n",
    "                         training_specimen_names]\n",
    "json_dict['test'] = [\"./imagesTr/%s.nii.gz\" % i.split(\"/\")[-1] for i in test_specimen_names]\n",
    "\n",
    "print(json_dict)\n",
    "with open(os.path.join(output_path, 'dataset.json'),'w+') as f:\n",
    "    json.dump(json_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
